{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Libraries\n",
    "# Scraping\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "# Visualizations and analysis\n",
    "import numpy as np\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as plt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from matplotlib.pyplot import imread\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HTML code\n",
    "url = 'https://www.indeed.com/jobs?q=information+technology&l=OH'\n",
    "page_response = requests.get(url, timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page_response.text, 'html5lib')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a', {'class':'jobtitle'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a', {'class':'jobtitle'}).text, soup.find('a', {'class':'jobtitle'})['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('span', {'class':'company'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('span', {'class':'company'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('span', {'class':'company'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in soup.find_all('span', {'class':'company'}):\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indeed_job_scrape(keyword, search_location, no_page, job_type='None', exp_lvl='None'):\n",
    "    ### 'keyword' transformation to fit in with url\n",
    "    keyword = keyword.replace(' ','+')\n",
    "    ### exp_level number convert to query arguement\n",
    "    if exp_lvl == 1:\n",
    "        exp_lvl_str = 'entry_level'\n",
    "    elif exp_lvl == 2:\n",
    "        exp_lvl_str = 'mid_level'\n",
    "    elif exp_lvl == 3:\n",
    "        exp_lvl_str = 'senior_level'\n",
    "    else:\n",
    "        raise ValueError('exp_lvl only accpets 1, 2, or 3')\n",
    "    \n",
    "    ### Data to scrape\n",
    "    # Job title\n",
    "    j_title = []\n",
    "    # Company name\n",
    "    company_name = []\n",
    "    # Salary\n",
    "    salary = []\n",
    "    # Location\n",
    "    location = []\n",
    "    # Rating\n",
    "    company_rating = []\n",
    "    # Posting link\n",
    "    hyperlink = []\n",
    "    # Posting description\n",
    "    j_desc = []\n",
    "    \n",
    "    ### Main scraping loop\n",
    "    for page_index in range(0, no_page*10, 10):\n",
    "        page = 'https://www.indeed.com/jobs?q=' + keyword + '&l=' + search_location + '&jt=' + job_type + '&explvl=' + exp_lvl_str + '&start=' + str(page_index)\n",
    "        print(page)\n",
    "        page_response = requests.get(page, timeout=5)\n",
    "        main_soup = BeautifulSoup(page_response.text, 'html5lib')\n",
    "        for i in main_soup.find_all('div', {'class':'jobsearch-SerpJobCard'}):\n",
    "            # Position title\n",
    "            j_title.append(i.find('a', {'class':'jobtitle'})['title']) \n",
    "            # Company name                       \n",
    "            company_name.append(i.find('span', {'class':'company'}).text)\n",
    "            # Salary (if information available, 'None' otherwise)                        \n",
    "            salary.append(i.find('span', {'class':'salaryText'}).text if i.find('span', {'class':'salaryText'}) else 'None')  \n",
    "            # Job location                             \n",
    "            location.append(i.find(attrs={'class':'location'}).text)\n",
    "            # Comapny rating\n",
    "            company_rating.append(i.find('span', {'class':'ratingsContent'}).text if i.find('span', {'class':'ratingsContent'}) else 'None')\n",
    "            # Link to detailed job posting\n",
    "            hyperlink.append('https://www.indeed.com/' + str(i.find('a', {'class':'jobtitle'})['href']))\n",
    "            # Fulljob description\n",
    "            url = 'https://www.indeed.com/' + str(i.find('a', {'class':'jobtitle'})['href'])\n",
    "            url_response = requests.get(url, timeout=5)\n",
    "            soupy_soup = BeautifulSoup(url_response.text, 'html5lib')\n",
    "            j_desc.append(soupy_soup.find('div', {'id':'jobDescriptionText'}).text)\n",
    "    \n",
    "    ### Save to pandas dataframe \n",
    "    df_local = pd.DataFrame({'job_title' : j_title,\n",
    "                       'company_name' : company_name,\n",
    "                       'salary' : salary,\n",
    "                       'job_location' : location,\n",
    "                       'direct_link' : hyperlink,\n",
    "                       'full_description' : j_desc})\n",
    "    return df_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = indeed_job_scrape('information technology', 'Ohio', no_page=1, exp_lvl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.full_description[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Demo analysis\n",
    "df = pd.read_csv('full_dataset_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of each job level\n",
    "plt.bar(10, len(df.exp_lvl[df.exp_lvl==1]), label='Entry Level')\n",
    "plt.bar(15, len(df.exp_lvl[df.exp_lvl==2]), label='Mid Level')\n",
    "plt.bar(20, len(df.exp_lvl[df.exp_lvl==3]), label='Senior Level')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring salary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of postings with salary information\n",
    "df.groupby('exp_lvl')[['yearly_avg_salary']].agg(lambda x: x.count()/(x.count()+x.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of availble salary rate at each level\n",
    "df.groupby('exp_lvl')[['yearly_avg_salary']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median\n",
    "df.groupby('exp_lvl')[['yearly_avg_salary']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word cloud\n",
    "stop_words = ['work', 'will', 'system', 'support'] + list(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for entry level job\n",
    "jd_dict_1 = df[df.exp_lvl==1].full_description.values\n",
    "j1_cloud = WordCloud(stopwords = stop_words,\n",
    "                     background_color='white',\n",
    "                     width=2500,\n",
    "                     height=1800).generate(' '.join(jd_dict_1))\n",
    "plt.imshow(j1_cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for mid level job\n",
    "jd_dict_2 = df[df.exp_lvl==2].full_description.values\n",
    "j2_cloud = WordCloud(stopwords = stop_words,\n",
    "                     background_color='white',\n",
    "                     width=2500,\n",
    "                     height=1800).generate(' '.join(jd_dict_2))\n",
    "plt.imshow(j2_cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for senior level job\n",
    "jd_dict_3 = df[df.exp_lvl==3].full_description.values\n",
    "j3_cloud = WordCloud(stopwords = stop_words,\n",
    "                     background_color='white',\n",
    "                     width=2500,\n",
    "                     height=1800).generate(' '.join(jd_dict_3))\n",
    "plt.imshow(j3_cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subclass sklearn tokenizer with nltk lemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "class LemmaCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(i) for i in analyzer(doc))\n",
    "\n",
    "# Applying the new tokenizer method\n",
    "tf_vectorizer = LemmaCountVectorizer(max_df=0.95, min_df=5, stop_words='english', decode_error='ignore')\n",
    "tf_vector = tf_vectorizer.fit_transform(list(df.full_description.values))\n",
    "\n",
    "# Top 50 frequent clean\n",
    "word_freq = zip(tf_vectorizer.get_feature_names(),\n",
    "                tf_vector.toarray().sum(axis=0))\n",
    "word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "plot_data_clean = [go.Bar(x=list(zip(*word_freq))[0][0:50],\n",
    "                   y=list(zip(*word_freq))[1][0:50],\n",
    "                   marker=dict(colorscale='Jet',\n",
    "                   color=list(zip(*word_freq))[1][0:50]\n",
    "                   ),\n",
    "                   text='Word Counts')]\n",
    "layout = go.Layout(title='Top 50 Frequent Words after Processing')                    \n",
    "fig = go.Figure(data=plot_data_clean, layout=layout)\n",
    "py.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
